{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:53:15.337557Z","iopub.execute_input":"2025-09-28T13:53:15.337771Z","iopub.status.idle":"2025-09-28T13:54:40.273304Z","shell.execute_reply.started":"2025-09-28T13:53:15.337743Z","shell.execute_reply":"2025-09-28T13:54:40.272453Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:54:40.275100Z","iopub.execute_input":"2025-09-28T13:54:40.275347Z","iopub.status.idle":"2025-09-28T13:54:40.542691Z","shell.execute_reply.started":"2025-09-28T13:54:40.275325Z","shell.execute_reply":"2025-09-28T13:54:40.541883Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n/kaggle/input/jigsaw-agile-community-rules/train.csv\n/kaggle/input/jigsaw-agile-community-rules/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport shutil\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom accelerate import PartialState\nfrom transformers import EarlyStoppingCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:54:40.543438Z","iopub.execute_input":"2025-09-28T13:54:40.543897Z","iopub.status.idle":"2025-09-28T13:55:15.466008Z","shell.execute_reply.started":"2025-09-28T13:54:40.543870Z","shell.execute_reply":"2025-09-28T13:55:15.465453Z"}},"outputs":[{"name":"stderr","text":"2025-09-28 13:54:56.986876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759067697.325496      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759067697.420897      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"distributed_state = PartialState()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:15.466690Z","iopub.execute_input":"2025-09-28T13:55:15.467146Z","iopub.status.idle":"2025-09-28T13:55:15.501290Z","shell.execute_reply.started":"2025-09-28T13:55:15.467127Z","shell.execute_reply":"2025-09-28T13:55:15.500721Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n\n# Combine text fields for each sample\ndef combine_text_fields(row):\n    return f\"Comment: {row['body']}\\nRule: {row['rule']}\\nPositive Example 1: {row['positive_example_1']}\\nPositive Example 2: {row['positive_example_2']}\\nNegative Example 1: {row['negative_example_1']}\\nNegative Example 2: {row['negative_example_2']}\"\n\ntrain_df['combined_text'] = train_df.apply(combine_text_fields, axis=1)\n\n# Prepare data for k-fold\ntexts = train_df['combined_text'].tolist()\nlabels = train_df['rule_violation'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:15.501965Z","iopub.execute_input":"2025-09-28T13:55:15.502205Z","iopub.status.idle":"2025-09-28T13:55:15.602687Z","shell.execute_reply.started":"2025-09-28T13:55:15.502187Z","shell.execute_reply":"2025-09-28T13:55:15.601958Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class RedditDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=1024):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:15.603484Z","iopub.execute_input":"2025-09-28T13:55:15.603703Z","iopub.status.idle":"2025-09-28T13:55:15.608611Z","shell.execute_reply.started":"2025-09-28T13:55:15.603686Z","shell.execute_reply":"2025-09-28T13:55:15.607926Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Initialize tokenizer\nmodel_name = \"answerdotai/ModernBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:15.610478Z","iopub.execute_input":"2025-09-28T13:55:15.610775Z","iopub.status.idle":"2025-09-28T13:55:16.488837Z","shell.execute_reply.started":"2025-09-28T13:55:15.610759Z","shell.execute_reply":"2025-09-28T13:55:16.488241Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4062db78c25141e9a41e8543858fda62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed618f7855b4d75b3a584b24091b68a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b72d2da4c904396a124f368efc8e045"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_name)\nconfig.hidden_dropout_prob = 0.3\nconfig.attention_probs_dropout_prob = 0.3\nconfig.num_labels = 2\nconfig.problem_type = \"single_label_classification\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.489634Z","iopub.execute_input":"2025-09-28T13:55:16.489908Z","iopub.status.idle":"2025-09-28T13:55:16.607183Z","shell.execute_reply.started":"2025-09-28T13:55:16.489883Z","shell.execute_reply":"2025-09-28T13:55:16.606690Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c0a5560aa84c119496479fd51646c8"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n    auc = roc_auc_score(labels, probs)\n    return {\"auc\": auc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.607760Z","iopub.execute_input":"2025-09-28T13:55:16.607985Z","iopub.status.idle":"2025-09-28T13:55:16.611841Z","shell.execute_reply.started":"2025-09-28T13:55:16.607964Z","shell.execute_reply":"2025-09-28T13:55:16.611160Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.training_losses = []\n        self.validation_losses = []\n    \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        labels = labels.long()\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n        \n        self.training_losses.append(loss.item())\n        \n        return (loss, outputs) if return_outputs else loss\n    \n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        eval_output = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n        \n        if \"eval_loss\" in eval_output:\n            self.validation_losses.append(eval_output[\"eval_loss\"])\n        \n        return eval_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.612489Z","iopub.execute_input":"2025-09-28T13:55:16.612691Z","iopub.status.idle":"2025-09-28T13:55:16.628964Z","shell.execute_reply.started":"2025-09-28T13:55:16.612677Z","shell.execute_reply":"2025-09-28T13:55:16.628446Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Analysis function\ndef analyze_kfold_performance(all_fold_results, train_texts_fold, training_args):\n    \"\"\"\n    Analyze model performance across k-fold cross-validation to detect underfitting/overfitting\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"K-FOLD CROSS-VALIDATION ANALYSIS\")\n    print(\"=\"*60)\n    \n    # Extract AUC scores\n    auc_scores = [result['auc'] for result in all_fold_results]\n    mean_auc = np.mean(auc_scores)\n    std_auc = np.std(auc_scores)\n    \n    print(f\"Mean AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n    print(f\"Individual Fold AUCs: {[f'{auc:.4f}' for auc in auc_scores]}\")\n    \n    # Analyze loss curves\n    max_epochs = max(len(result['val_loss']) for result in all_fold_results)\n    avg_train_loss = np.zeros(max_epochs)\n    avg_val_loss = np.zeros(max_epochs)\n    epoch_counts = np.zeros(max_epochs)\n    \n    # Calculate average losses per epoch\n    for result in all_fold_results:\n        train_losses = result['train_loss']\n        val_losses = result['val_loss']\n        \n        # Group training losses by epoch\n        epoch_length = len(train_texts_fold) // training_args.per_device_train_batch_size\n        train_losses_per_epoch = [train_losses[i:i+epoch_length] for i in range(0, len(train_losses), epoch_length)]\n        \n        for epoch, (train_epoch_losses, val_loss) in enumerate(zip(train_losses_per_epoch, val_losses)):\n            if epoch < max_epochs:\n                avg_train_loss[epoch] += np.mean(train_epoch_losses)\n                avg_val_loss[epoch] += val_loss\n                epoch_counts[epoch] += 1\n    \n    # Calculate averages\n    for epoch in range(max_epochs):\n        if epoch_counts[epoch] > 0:\n            avg_train_loss[epoch] /= epoch_counts[epoch]\n            avg_val_loss[epoch] /= epoch_counts[epoch]\n    \n    # Find best epoch\n    best_epoch = np.argmin(avg_val_loss)\n    best_train_loss = avg_train_loss[best_epoch]\n    best_val_loss = avg_val_loss[best_epoch]\n    \n    print(f\"\\nBest Performance at Epoch {best_epoch+1}:\")\n    print(f\"Training Loss: {best_train_loss:.4f}\")\n    print(f\"Validation Loss: {best_val_loss:.4f}\")\n    print(f\"Loss Gap (Val - Train): {best_val_loss - best_train_loss:.4f}\")\n    \n    # Analyze underfitting/overfitting\n    print(\"\\n\" + \"-\"*60)\n    print(\"MODEL STATUS ANALYSIS\")\n    print(\"-\"*60)\n    \n    # Check for underfitting\n    if best_train_loss > 0.5 and best_val_loss > 0.5:\n        print(\"Status: UNDERFITTING\")\n        print(\"Reasons: Both training and validation losses are high\")\n        print(\"Recommendations:\")\n        print(\"- Increase model capacity (use larger model)\")\n        print(\"- Train for more epochs\")\n        print(\"- Reduce regularization\")\n        print(\"- Increase learning rate\")\n    \n    # Check for overfitting\n    elif (best_val_loss - best_train_loss) > 0.1:\n        print(\"Status: OVERFITTING\")\n        print(\"Reasons: Validation loss is significantly higher than training loss\")\n        print(\"Recommendations:\")\n        print(\"- Add more dropout\")\n        print(\"- Increase weight decay\")\n        print(\"- Use early stopping with lower patience\")\n        print(\"- Add data augmentation\")\n        print(\"- Reduce model complexity\")\n    \n    # Check for good fit\n    elif best_train_loss < 0.4 and best_val_loss < 0.4 and abs(best_val_loss - best_train_loss) < 0.05:\n        print(\"Status: GOOD FIT\")\n        print(\"Reasons: Both losses are low and close to each other\")\n        print(\"Recommendations:\")\n        print(\"- Continue monitoring for overfitting\")\n        print(\"- Consider fine-tuning hyperparameters\")\n        print(\"- Try ensemble methods for further improvement\")\n    \n    # Check for under-optimized\n    else:\n        print(\"Status: UNDER-OPTIMIZED\")\n        print(\"Reasons: Model has potential but needs more optimization\")\n        print(\"Recommendations:\")\n        print(\"- Train for more epochs\")\n        print(\"- Adjust learning rate\")\n        print(\"- Try different optimizers\")\n        print(\"- Tune hyperparameters more carefully\")\n    \n    # Analyze consistency across folds\n    print(\"\\n\" + \"-\"*60)\n    print(\"FOLD CONSISTENCY ANALYSIS\")\n    print(\"-\"*60)\n    \n    if std_auc < 0.02:\n        print(\"Consistency: VERY HIGH\")\n        print(\"The model performs consistently across all folds\")\n    elif std_auc < 0.05:\n        print(\"Consistency: HIGH\")\n        print(\"The model performs quite consistently across folds\")\n    elif std_auc < 0.1:\n        print(\"Consistency: MODERATE\")\n        print(\"There's some variation in performance across folds\")\n    else:\n        print(\"Consistency: LOW\")\n        print(\"Performance varies significantly across folds\")\n        print(\"Recommendations:\")\n        print(\"- Check for data distribution issues\")\n        print(\"- Consider stratified sampling\")\n        print(\"- Increase training data\")\n    \n    # Plot loss curves\n    plt.figure(figsize=(12, 6))\n    sns.set(style=\"whitegrid\")\n    \n    epochs = range(1, max_epochs + 1)\n    sns.lineplot(x=epochs, y=avg_train_loss, label='Average Training Loss', marker='o', linewidth=2)\n    sns.lineplot(x=epochs, y=avg_val_loss, label='Average Validation Loss', marker='s', linewidth=2)\n    \n    # Mark best epoch\n    plt.axvline(x=best_epoch+1, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n    \n    plt.title('Average Training vs Validation Loss Across Folds', fontsize=16)\n    plt.xlabel('Epoch', fontsize=14)\n    plt.ylabel('Loss', fontsize=14)\n    plt.xticks(epochs)\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    plt.savefig('./loss_analysis.png')\n    plt.show()\n    \n    # Plot individual fold performances\n    plt.figure(figsize=(12, 6))\n    sns.set(style=\"whitegrid\")\n    \n    fold_nums = [result['fold'] for result in all_fold_results]\n    fold_aucs = [result['auc'] for result in all_fold_results]\n    \n    sns.barplot(x=fold_nums, y=fold_aucs, palette='viridis')\n    plt.axhline(y=mean_auc, color='red', linestyle='--', label=f'Mean AUC ({mean_auc:.4f})')\n    \n    plt.title('AUC Scores Across Folds', fontsize=16)\n    plt.xlabel('Fold', fontsize=14)\n    plt.ylabel('AUC', fontsize=14)\n    plt.ylim(0.5, 1.0)  # AUC range\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    plt.savefig('./fold_performance.png')\n    plt.show()\n    \n    return {\n        'mean_auc': mean_auc,\n        'std_auc': std_auc,\n        'best_epoch': best_epoch,\n        'best_train_loss': best_train_loss,\n        'best_val_loss': best_val_loss,\n        'loss_gap': best_val_loss - best_train_loss\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.629630Z","iopub.execute_input":"2025-09-28T13:55:16.629919Z","iopub.status.idle":"2025-09-28T13:55:16.654936Z","shell.execute_reply.started":"2025-09-28T13:55:16.629896Z","shell.execute_reply":"2025-09-28T13:55:16.654447Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"n_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Track best model\nbest_auc = 0\nbest_fold = None\nbest_model_path = None\n\n# Store results for analysis\nall_fold_results = []\n\n# Create output directory\nos.makedirs(\"./results\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.655533Z","iopub.execute_input":"2025-09-28T13:55:16.655775Z","iopub.status.idle":"2025-09-28T13:55:16.677351Z","shell.execute_reply.started":"2025-09-28T13:55:16.655750Z","shell.execute_reply":"2025-09-28T13:55:16.676676Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n    print(f\"\\n{'='*50}\")\n    print(f\"Training Fold {fold+1}/{n_splits}\")\n    print(f\"{'='*50}\")\n    \n    # Split data for this fold\n    train_texts_fold = [texts[i] for i in train_idx]\n    train_labels_fold = [labels[i] for i in train_idx]\n    val_texts_fold = [texts[i] for i in val_idx]\n    val_labels_fold = [labels[i] for i in val_idx]\n    \n    # Create datasets\n    train_dataset = RedditDataset(train_texts_fold, train_labels_fold, tokenizer)\n    val_dataset = RedditDataset(val_texts_fold, val_labels_fold, tokenizer)\n    \n    # Initialize model for this fold\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        config=config\n    )\n    \n    # Set up output directory for this fold\n    output_dir = f\"./results/fold_{fold}\"\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        gradient_accumulation_steps=2,\n        learning_rate=1e-5,\n        warmup_ratio=0.1,\n        weight_decay=0.1,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=100,\n        eval_strategy=\"epoch\",\n        save_strategy=\"best\",\n        save_total_limit=1,   \n        load_best_model_at_end=True,\n        metric_for_best_model=\"auc\",\n        greater_is_better=True,\n        report_to=\"none\",\n        fp16=True,\n        bf16=False,\n        local_rank=-1,\n        ddp_find_unused_parameters=False,\n    )\n    \n    # Initialize trainer\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Evaluate to get the AUC for this fold\n    eval_result = trainer.evaluate()\n    fold_auc = eval_result[\"eval_auc\"]\n    print(f\"Fold {fold+1} AUC: {fold_auc:.4f}\")\n    \n    # Store results\n    all_fold_results.append({\n        'fold': fold+1,\n        'auc': fold_auc,\n        'train_loss': trainer.training_losses,\n        'val_loss': trainer.validation_losses\n    })\n    \n    # If this fold's AUC is the best, save this model\n    if fold_auc > best_auc:\n        best_auc = fold_auc\n        best_fold = fold\n        \n        # Remove previous best model if exists\n        if best_model_path and os.path.exists(best_model_path):\n            shutil.rmtree(best_model_path)\n        \n        # Save the best model\n        best_model_path = \"./best_model\"\n        trainer.save_model(best_model_path)\n        tokenizer.save_pretrained(best_model_path)\n        \n        print(f\"New best model found! Fold {fold+1} with AUC: {fold_auc:.4f}\")\n    \n    # Clean up fold directory to save space\n    if os.path.exists(output_dir):\n        shutil.rmtree(output_dir)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"K-Fold Cross-Validation Complete\")\nprint(f\"{'='*50}\")\nprint(f\"Best Fold: {best_fold+1}\")\nprint(f\"Best AUC: {best_auc:.4f}\")\nprint(f\"Best model saved to: {best_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T13:55:16.678097Z","iopub.execute_input":"2025-09-28T13:55:16.678315Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTraining Fold 1/5\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"777efd281c96498bbc90da594c95c340"}},"metadata":{}},{"name":"stderr","text":"Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/2024326281.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\nW0928 13:55:32.499000 36 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1218' max='2030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1218/2030 47:58 < 32:02, 0.42 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.700500</td>\n      <td>0.678283</td>\n      <td>0.679769</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.672300</td>\n      <td>0.614264</td>\n      <td>0.772646</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.530500</td>\n      <td>0.559631</td>\n      <td>0.801383</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.431700</td>\n      <td>0.719150</td>\n      <td>0.824697</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.262400</td>\n      <td>1.094878</td>\n      <td>0.826456</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.177100</td>\n      <td>1.570823</td>\n      <td>0.817621</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [26/26 00:36]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Fold 1 AUC: 0.8176\nNew best model found! Fold 1 with AUC: 0.8176\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining Fold 2/5\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2024326281.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1015' max='2030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1015/2030 40:01 < 40:06, 0.42 it/s, Epoch 5/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.699300</td>\n      <td>0.685863</td>\n      <td>0.649417</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.613800</td>\n      <td>0.592927</td>\n      <td>0.739867</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.484900</td>\n      <td>0.598645</td>\n      <td>0.775085</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.346500</td>\n      <td>0.731315</td>\n      <td>0.785813</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.236400</td>\n      <td>1.483401</td>\n      <td>0.774333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [26/26 00:36]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Fold 2 AUC: 0.7743\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining Fold 3/5\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2024326281.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1016' max='2030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1016/2030 38:51 < 38:51, 0.43 it/s, Epoch 5/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.705600</td>\n      <td>0.675268</td>\n      <td>0.617913</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.629600</td>\n      <td>0.588386</td>\n      <td>0.774612</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.505900</td>\n      <td>0.552506</td>\n      <td>0.812779</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.352500</td>\n      <td>0.718070</td>\n      <td>0.829854</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='4' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4/26 00:04 < 00:32, 0.67 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Analyze performance\nanalysis_results = analyze_kfold_performance(all_fold_results, train_texts_fold, training_args)\n\n# Print summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Best Model: Fold {best_fold+1}\")\nprint(f\"Best AUC: {best_auc:.4f}\")\nprint(f\"Mean AUC across folds: {analysis_results['mean_auc']:.4f}\")\nprint(f\"Best Epoch: {analysis_results['best_epoch']+1}\")\nprint(f\"Loss Gap: {analysis_results['loss_gap']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save analysis results\nwith open('./analysis_results.txt', 'w') as f:\n    f.write(f\"Best Fold: {best_fold+1}\\n\")\n    f.write(f\"Best AUC: {best_auc:.4f}\\n\")\n    f.write(f\"Mean AUC: {analysis_results['mean_auc']:.4f}\\n\")\n    f.write(f\"Std AUC: {analysis_results['std_auc']:.4f}\\n\")\n    f.write(f\"Best Epoch: {analysis_results['best_epoch']+1}\\n\")\n    f.write(f\"Best Training Loss: {analysis_results['best_train_loss']:.4f}\\n\")\n    f.write(f\"Best Validation Loss: {analysis_results['best_val_loss']:.4f}\\n\")\n    f.write(f\"Loss Gap: {analysis_results['loss_gap']:.4f}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":593444,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442749,"modelId":459283}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:48:15.512732Z","iopub.execute_input":"2025-09-27T14:48:15.512943Z","iopub.status.idle":"2025-09-27T14:48:17.781905Z","shell.execute_reply.started":"2025-09-27T14:48:15.512924Z","shell.execute_reply":"2025-09-27T14:48:17.780972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom tqdm import tqdm\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:48:17.784063Z","iopub.execute_input":"2025-09-27T14:48:17.784482Z","iopub.status.idle":"2025-09-27T14:48:28.340480Z","shell.execute_reply.started":"2025-09-27T14:48:17.784458Z","shell.execute_reply":"2025-09-27T14:48:28.339432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# URL keyword extraction function (same as in training)\ndef url_to_semantics(text):\n    \"\"\"\n    Extract meaningful keywords from URLs in text.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Regular expression to match URLs\n    url_pattern = re.compile(\n        r'https?://'  # http:// or https://\n        r'(?:www\\.)?'  # optional www.\n        r'([^/?]+)'  # domain (group 1)\n        r'(?:[^/]*)'  # optional TLD and port\n        r'(/[^/?]*)'  # path (group 2)\n    )\n    \n    urls = url_pattern.findall(text)\n    \n    if not urls:\n        return \"\"\n    \n    keywords = []\n    \n    for domain, path in urls:\n        # Clean domain: remove TLD and common prefixes\n        domain = domain.split('.')[0]  # Take first part before dot\n        if domain and domain not in ['www', 'http', 'https']:\n            keywords.append(f\"domain:{domain}\")\n        \n        # Clean path: remove leading slash and split\n        if path and len(path) > 1:  # Ensure path is not just \"/\"\n            path_parts = path.strip('/').split('/')\n            for part in path_parts:\n                # Skip empty parts, numbers, or very short parts\n                if part and not part.isdigit() and len(part) > 2:\n                    # Skip common file extensions\n                    if part.lower() not in ['jpg', 'jpeg', 'png', 'gif', 'html', 'php', 'asp', 'aspx']:\n                        keywords.append(f\"path:{part}\")\n                        break  # Only take the first meaningful path part\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_keywords = []\n    for kw in keywords:\n        if kw not in seen:\n            seen.add(kw)\n            unique_keywords.append(kw)\n    \n    if unique_keywords:\n        return \"URL Keywords: \" + \" \".join(unique_keywords)\n    else:\n        return \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n\n# Apply URL keyword extraction\ntest_df[\"body_with_url\"] = test_df[\"body\"].apply(lambda x: x + \" \" + url_to_semantics(x))\n\n# Create combined text with rule and body_with_url\ntest_df['combined_text'] = test_df[\"rule\"] + \"[SEP]\" + test_df[\"body_with_url\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:48:28.341497Z","iopub.execute_input":"2025-09-27T14:48:28.341920Z","iopub.status.idle":"2025-09-27T14:48:28.365711Z","shell.execute_reply.started":"2025-09-27T14:48:28.341899Z","shell.execute_reply":"2025-09-27T14:48:28.364664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create test dataset\nclass TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=512):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/input/modernbert-base-trained/pytorch/default/3/model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/modernbert-base-trained/pytorch/default/3/model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:48:28.374280Z","iopub.execute_input":"2025-09-27T14:48:28.374589Z","iopub.status.idle":"2025-09-27T14:48:52.553092Z","shell.execute_reply.started":"2025-09-27T14:48:28.374568Z","shell.execute_reply":"2025-09-27T14:48:52.552073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create test dataset\ntest_dataset = TestDataset(test_df['combined_text'].tolist(), tokenizer)\n\n# Prediction function\ndef predict_with_model(model, test_dataset, batch_size=16):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    \n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    all_predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Predicting\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            \n            # For binary classification, get probability of positive class\n            probs = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n            all_predictions.extend(probs)\n    \n    return all_predictions\n\n# Get predictions\npredictions = predict_with_model(model, test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:48:52.554051Z","iopub.execute_input":"2025-09-27T14:48:52.554734Z","iopub.status.idle":"2025-09-27T14:49:11.264363Z","shell.execute_reply.started":"2025-09-27T14:48:52.554708Z","shell.execute_reply":"2025-09-27T14:49:11.263489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'row_id': test_df['row_id'],\n    'rule_violation': predictions\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:49:11.266208Z","iopub.execute_input":"2025-09-27T14:49:11.266493Z","iopub.status.idle":"2025-09-27T14:49:11.280270Z","shell.execute_reply.started":"2025-09-27T14:49:11.266472Z","shell.execute_reply":"2025-09-27T14:49:11.279250Z"}},"outputs":[],"execution_count":null}]}
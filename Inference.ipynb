{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":596176,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442749,"modelId":459283}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:45.197718Z","iopub.execute_input":"2025-10-02T18:02:45.197934Z","iopub.status.idle":"2025-10-02T18:02:46.883666Z","shell.execute_reply.started":"2025-10-02T18:02:45.197915Z","shell.execute_reply":"2025-10-02T18:02:46.882978Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modernbert-base-trained/pytorch/default/4/model/config.json\n/kaggle/input/modernbert-base-trained/pytorch/default/4/model/training_args.bin\n/kaggle/input/modernbert-base-trained/pytorch/default/4/model/tokenizer.json\n/kaggle/input/modernbert-base-trained/pytorch/default/4/model/tokenizer_config.json\n/kaggle/input/modernbert-base-trained/pytorch/default/4/model/model.safetensors\n/kaggle/input/modernbert-base-trained/pytorch/default/4/model/special_tokens_map.json\n/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n/kaggle/input/jigsaw-agile-community-rules/train.csv\n/kaggle/input/jigsaw-agile-community-rules/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom tqdm import tqdm\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:46.885076Z","iopub.execute_input":"2025-10-02T18:02:46.885390Z","iopub.status.idle":"2025-10-02T18:02:55.263674Z","shell.execute_reply.started":"2025-10-02T18:02:46.885372Z","shell.execute_reply":"2025-10-02T18:02:55.263155Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def url_to_semantics(text):\n    \"\"\"Extract meaningful keywords from URLs in text and capitalize them.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    url_pattern = re.compile(r'https?://(?:www\\.)?([^/?]+)(?:[^/]*)(/[^/?]*)')\n    urls = url_pattern.findall(text)\n    if not urls:\n        return \"\"\n    keywords = []\n    for domain, path in urls:\n        domain = domain.split('.')[0]\n        if domain and domain not in ['www', 'http', 'https']:\n            keywords.append(f\"domain:{domain}\")\n        if path and len(path) > 1:\n            path_parts = path.strip('/').split('/')\n            for part in path_parts:\n                if part and not part.isdigit() and len(part) > 2:\n                    if part.lower() not in ['jpg', 'jpeg', 'png', 'gif', 'html', 'php', 'asp', 'aspx']:\n                        keywords.append(f\"path:{part}\")\n                        break\n    seen = set()\n    unique_keywords = []\n    for kw in keywords:\n        if kw not in seen:\n            seen.add(kw)\n            unique_keywords.append(kw)\n    if unique_keywords:\n        capitalized_keywords = [kw.upper() for kw in unique_keywords]\n        return \"URL KEYWORDS: \" + \" \".join(capitalized_keywords)\n    else:\n        return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.264319Z","iopub.execute_input":"2025-10-02T18:02:55.264624Z","iopub.status.idle":"2025-10-02T18:02:55.271092Z","shell.execute_reply.started":"2025-10-02T18:02:55.264601Z","shell.execute_reply":"2025-10-02T18:02:55.270371Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def extract_keywords(text):\n    if not isinstance(text, str):\n        return []\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(text.lower())\n    keywords = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n    return keywords\n\ndef calculate_jaccard_similarity(list1, list2):\n    set1, set2 = set(list1), set(list2)\n    intersection, union = set1.intersection(set2), set1.union(set2)\n    return len(intersection) / len(union) if union else 0\n\ndef get_sentiment_features(text):\n    if not isinstance(text, str):\n        return 0, 0\n    sia = SentimentIntensityAnalyzer()\n    scores = sia.polarity_scores(text)\n    return scores['compound'], scores['pos'] - scores['neg']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.271856Z","iopub.execute_input":"2025-10-02T18:02:55.272105Z","iopub.status.idle":"2025-10-02T18:02:55.308181Z","shell.execute_reply.started":"2025-10-02T18:02:55.272078Z","shell.execute_reply":"2025-10-02T18:02:55.307529Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def create_example_features(row):\n    pos1_keywords = extract_keywords(row['positive_example_1'])\n    pos2_keywords = extract_keywords(row['positive_example_2'])\n    neg1_keywords = extract_keywords(row['negative_example_1'])\n    neg2_keywords = extract_keywords(row['negative_example_2'])\n    all_pos_keywords = pos1_keywords + pos2_keywords\n    all_neg_keywords = neg1_keywords + neg2_keywords\n    comment_keywords = extract_keywords(row['body'])\n    pos_overlap = calculate_jaccard_similarity(comment_keywords, all_pos_keywords)\n    neg_overlap = calculate_jaccard_similarity(comment_keywords, all_neg_keywords)\n    comment_compound, comment_sent_diff = get_sentiment_features(row['body'])\n    pos1_compound, _ = get_sentiment_features(row['positive_example_1'])\n    pos2_compound, _ = get_sentiment_features(row['positive_example_2'])\n    neg1_compound, _ = get_sentiment_features(row['negative_example_1'])\n    neg2_compound, _ = get_sentiment_features(row['negative_example_2'])\n    avg_pos_sentiment = (pos1_compound + pos2_compound) / 2\n    avg_neg_sentiment = (neg1_compound + neg2_compound) / 2\n    pos_sent_diff = abs(comment_compound - avg_pos_sentiment)\n    neg_sent_diff = abs(comment_compound - avg_neg_sentiment)\n    comment_len = len(row['body'].split())\n    pos1_len = len(row['positive_example_1'].split())\n    pos2_len = len(row['positive_example_2'].split())\n    neg1_len = len(row['negative_example_1'].split())\n    neg2_len = len(row['negative_example_2'].split())\n    avg_pos_len = (pos1_len + pos2_len) / 2\n    avg_neg_len = (neg1_len + neg2_len) / 2\n    pos_len_ratio = comment_len / avg_pos_len if avg_pos_len > 0 else 0\n    neg_len_ratio = comment_len / avg_neg_len if avg_neg_len > 0 else 0\n    features = (\n        f\"Example Features: \"\n        f\"pos_overlap:{pos_overlap:.3f} \"\n        f\"neg_overlap:{neg_overlap:.3f} \"\n        f\"pos_sent_diff:{pos_sent_diff:.3f} \"\n        f\"neg_sent_diff:{neg_sent_diff:.3f} \"\n        f\"pos_len_ratio:{pos_len_ratio:.3f} \"\n        f\"neg_len_ratio:{neg_len_ratio:.3f} \"\n        f\"comment_sentiment:{comment_sent_diff:.3f}\"\n    )\n    return features\n\ndef create_combined_text(row):\n    \"\"\"\n    Creates a combined text string using the hybrid approach from training.\n    This is the KEY function that must match training exactly.\n    \"\"\"\n    subreddit_text = str(row['subreddit']).upper()\n    rule_text = str(row['rule']).upper()\n    pos_examples = f\"{str(row['positive_example_1']).upper()} {str(row['positive_example_2']).upper()}\"\n    neg_examples = f\"{str(row['negative_example_1']).upper()} {str(row['negative_example_2']).upper()}\"\n    body_text = str(row['body']).upper()\n    url_info = url_to_semantics(row['body'])\n    features_str = create_example_features(row)\n    combined_text = (\n        f\"SUBREDDIT: {subreddit_text} [SEP] \"\n        f\"RULE: {rule_text} [SEP] \"\n        f\"POSITIVE EXAMPLES: {pos_examples} [SEP] \"\n        f\"NEGATIVE EXAMPLES: {neg_examples} [SEP] \"\n        f\"COMMENT: {body_text} {url_info} [SEP] \"\n        f\"{features_str.upper()}\"\n    )\n    return combined_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.308770Z","iopub.execute_input":"2025-10-02T18:02:55.308971Z","iopub.status.idle":"2025-10-02T18:02:55.324784Z","shell.execute_reply.started":"2025-10-02T18:02:55.308955Z","shell.execute_reply":"2025-10-02T18:02:55.324079Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load test data\ntest_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n\nprint(\"Creating combined text for inference...\")\ntest_df['combined_text'] = test_df.apply(create_combined_text, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.325513Z","iopub.execute_input":"2025-10-02T18:02:55.325837Z","iopub.status.idle":"2025-10-02T18:02:55.645535Z","shell.execute_reply.started":"2025-10-02T18:02:55.325797Z","shell.execute_reply":"2025-10-02T18:02:55.644828Z"}},"outputs":[{"name":"stdout","text":"Creating combined text for inference...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create test dataset\nclass TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=1024):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.647655Z","iopub.execute_input":"2025-10-02T18:02:55.647839Z","iopub.status.idle":"2025-10-02T18:02:55.652522Z","shell.execute_reply.started":"2025-10-02T18:02:55.647824Z","shell.execute_reply":"2025-10-02T18:02:55.651731Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/input/modernbert-base-trained/pytorch/default/4/model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/modernbert-base-trained/pytorch/default/4/model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:02:55.653073Z","iopub.execute_input":"2025-10-02T18:02:55.653341Z","iopub.status.idle":"2025-10-02T18:03:14.774429Z","shell.execute_reply.started":"2025-10-02T18:02:55.653321Z","shell.execute_reply":"2025-10-02T18:03:14.773625Z"}},"outputs":[{"name":"stderr","text":"2025-10-02 18:03:01.527322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759428181.694648      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759428181.751561      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Create test dataset\ntest_dataset = TestDataset(test_df['combined_text'].tolist(), tokenizer)\n\n# Prediction function\ndef predict_with_model(model, test_dataset, batch_size=16):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    all_predictions = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Predicting\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            probs = torch.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n            all_predictions.extend(probs)\n    return all_predictions\n\n# Get predictions\npredictions = predict_with_model(model, test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:03:14.775199Z","iopub.execute_input":"2025-10-02T18:03:14.775720Z","iopub.status.idle":"2025-10-02T18:03:26.893465Z","shell.execute_reply.started":"2025-10-02T18:03:14.775701Z","shell.execute_reply":"2025-10-02T18:03:26.892848Z"}},"outputs":[{"name":"stderr","text":"Predicting:   0%|          | 0/1 [00:00<?, ?it/s]W1002 18:03:25.032000 36 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\nPredicting: 100%|██████████| 1/1 [00:08<00:00,  8.57s/it]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'row_id': test_df['row_id'],\n    'rule_violation': predictions\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"✅ Submission file created successfully with updated preprocessing!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T18:03:26.894136Z","iopub.execute_input":"2025-10-02T18:03:26.894406Z","iopub.status.idle":"2025-10-02T18:03:26.905082Z","shell.execute_reply.started":"2025-10-02T18:03:26.894381Z","shell.execute_reply":"2025-10-02T18:03:26.904518Z"}},"outputs":[{"name":"stdout","text":"✅ Submission file created successfully with updated preprocessing!\n","output_type":"stream"}],"execution_count":10}]}